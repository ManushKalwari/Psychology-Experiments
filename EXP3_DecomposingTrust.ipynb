{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8e4153-720c-42f7-bc57-4417ae6a9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Literal\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e259e8-2bd5-4cf3-9a8c-2b9b3bfdc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ParticipantProfile:\n",
    "    pid: str\n",
    "    demographic_group_no: int\n",
    "    location: str\n",
    "    occupation: str\n",
    "    trust_role: Optional[Literal[\"trustor\", \"trustee\"]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2e369c-18c7-4df3-8dea-32cde2848176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_participants_decomposing(exp_json: dict) -> List[ParticipantProfile]:\n",
    "    participants = []\n",
    "    pid_counter = 0\n",
    "\n",
    "    for group in exp_json[\"demographic_info\"]:\n",
    "        n = group[\"num_of_participants\"]\n",
    "        group_no = group[\"demographic_group_no\"]\n",
    "\n",
    "        # extract nested demographic info\n",
    "        location = None\n",
    "        occupation = None\n",
    "\n",
    "        for block in group[\"demographic_info\"]:\n",
    "            if block[\"category\"] == \"location\":\n",
    "                location = block[\"category_info\"][0][\"subcategory\"]\n",
    "            elif block[\"category\"] == \"occupation\":\n",
    "                occupation = block[\"category_info\"][0][\"subcategory\"]\n",
    "\n",
    "        assert location is not None\n",
    "        assert occupation is not None\n",
    "\n",
    "        for _ in range(n):\n",
    "            participants.append(\n",
    "                ParticipantProfile(\n",
    "                    pid=f\"P{pid_counter:04d}\",\n",
    "                    demographic_group_no=group_no,\n",
    "                    location=location,\n",
    "                    occupation=occupation,\n",
    "                )\n",
    "            )\n",
    "            pid_counter += 1\n",
    "\n",
    "    assert len(participants) == exp_json[\"total_participants_count\"]\n",
    "    return participants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba313436-e0d6-4216-a6a6-a8873e6663f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_trust_roles(participants: List[ParticipantProfile], seed: int = 0):\n",
    "    random.seed(seed)\n",
    "\n",
    "    by_group = {}\n",
    "    for p in participants:\n",
    "        by_group.setdefault(p.demographic_group_no, []).append(p)\n",
    "\n",
    "    for group_no, group_participants in by_group.items():\n",
    "        random.shuffle(group_participants)\n",
    "\n",
    "        half = len(group_participants) // 2\n",
    "        trustors = group_participants[:half]\n",
    "        trustees = group_participants[half:]\n",
    "\n",
    "        for p in trustors:\n",
    "            p.trust_role = \"trustor\"\n",
    "        for p in trustees:\n",
    "            p.trust_role = \"trustee\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b67dca-50a9-4d6b-8c82-5b9bb1fd3d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 359\n",
      "By site: Counter({'Capetown, South Africa': 129, 'Moscow, Russia': 118, 'Boston, United States': 112})\n",
      "Trust roles: Counter({'trustee': 180, 'trustor': 179})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"experiments_v3.json\", \"r\") as f:\n",
    "    all_experiments = json.load(f)\n",
    "\n",
    "EXP = next(\n",
    "    e for e in all_experiments\n",
    "    if e[\"experiment_id\"] == \"decomposing_trust_2006\"\n",
    ")\n",
    "\n",
    "\n",
    "participants = build_participants_decomposing(EXP)\n",
    "assign_trust_roles(participants, seed=42)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Total:\", len(participants))\n",
    "print(\"By site:\", Counter(p.location for p in participants))\n",
    "print(\"Trust roles:\", Counter(p.trust_role for p in participants))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474fb8a5-015e-4f0e-af42-dc5def684d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daffeccb-d5bc-466e-9cd1-414c3ffcc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecomposingTrustState:\n",
    "    participant: ParticipantProfile\n",
    "    transcript: List[Dict] = field(default_factory=list)\n",
    "\n",
    "    # Trust Game ONLY (operational, needed during generation)\n",
    "    trust_role: Optional[Literal[\"trustor\", \"trustee\"]] = None\n",
    "    trust_send_operational: Optional[int] = None             # trustor only\n",
    "    trust_expect_operational: Optional[int] = None           # trustor belief\n",
    "    trust_return_schedule_operational: Optional[Dict] = None # trustee only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cfb55a-d260-4d4f-9d56-821260477065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_states(participants: List[ParticipantProfile]) -> List[DecomposingTrustState]:\n",
    "    states = []\n",
    "    for p in participants:\n",
    "        s = DecomposingTrustState(\n",
    "            participant=p,\n",
    "            trust_role=p.trust_role\n",
    "        )\n",
    "        states.append(s)\n",
    "    return states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f314b531-fa4e-42b5-84cf-3c2c25808870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictator_prompt(state: DecomposingTrustState) -> str:\n",
    "    p = state.participant\n",
    "    return f\"\"\"\n",
    "You are participating in an economics experiment.\n",
    "\n",
    "About you:\n",
    "- Location: {p.location}\n",
    "- Occupation: {p.occupation}\n",
    "\n",
    "You are given 100 units of money.\n",
    "You may give any amount between 0 and 100 to another participant.\n",
    "The other participant cannot respond.\n",
    "\n",
    "Please:\n",
    "1) Decide how much you want to give.\n",
    "2) Briefly explain your reason in one or two sentences.\n",
    "\n",
    "Respond naturally, as a real participant would.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018e8476-36b9-44f1-b772-65e7c7d393b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_dictator_prompt(state: DecomposingTrustState) -> str:\n",
    "    p = state.participant\n",
    "    return f\"\"\"\n",
    "You are participating in an economics experiment.\n",
    "\n",
    "About you:\n",
    "- Location: {p.location}\n",
    "- Occupation: {p.occupation}\n",
    "\n",
    "You are given 100 units of money.\n",
    "Any amount you give will be tripled before the other participant receives it.\n",
    "The other participant cannot respond.\n",
    "\n",
    "Please:\n",
    "1) Decide how much you want to give.\n",
    "2) Briefly explain your reason in one or two sentences.\n",
    "\n",
    "Respond naturally, as a real participant would.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7004220-3f37-43d8-8808-d373dedeb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trustor_prompt(state: DecomposingTrustState) -> str:\n",
    "    p = state.participant\n",
    "    return f\"\"\"\n",
    "You are participating in an economics experiment.\n",
    "\n",
    "About you:\n",
    "- Location: {p.location}\n",
    "- Occupation: {p.occupation}\n",
    "\n",
    "You are Player 1.\n",
    "\n",
    "You are given 100 units of money.\n",
    "You may send any amount between 0 and 100 to Player 2.\n",
    "\n",
    "Any amount you send will be tripled before Player 2 receives it.\n",
    "Player 2 may later return some amount to you.\n",
    "\n",
    "Please:\n",
    "1) Decide how much money you want to send.\n",
    "2) State how much you expect Player 2 to return to you.\n",
    "3) Briefly explain your reasoning in one or two sentences.\n",
    "\n",
    "Respond naturally, as a real participant would.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e86509c9-d579-467c-9dcd-db3c01dc2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trustee_prompt(state: DecomposingTrustState) -> str:\n",
    "    p = state.participant\n",
    "    return f\"\"\"\n",
    "You are participating in an economics experiment.\n",
    "\n",
    "About you:\n",
    "- Location: {p.location}\n",
    "- Occupation: {p.occupation}\n",
    "\n",
    "You are Player 2.\n",
    "\n",
    "Player 1 may send you any amount between 0 and 100 units.\n",
    "Any amount sent to you will be tripled before you receive it.\n",
    "\n",
    "Think carefully about how you would respond to different levels of trust.\n",
    "\n",
    "For each of the following amounts that Player 1 could send:\n",
    "10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "\n",
    "Please state how much you would return to Player 1 after receiving the tripled amount.\n",
    "\n",
    "Important:\n",
    "- Please give a decision for **every amount listed above**.\n",
    "- You may explain briefly how your returns change as the sent amount increases.\n",
    "- Make sure none of the listed amounts are skipped.\n",
    "\n",
    "Respond as a real participant would.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8a55fb-d2e0-4a24-99c9-c812f9ac3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_prompt(state: DecomposingTrustState) -> str:\n",
    "    p = state.participant\n",
    "    sure_amounts = [40, 60, 80, 100, 120, 140]\n",
    "\n",
    "    lines = \"\\n\".join(\n",
    "        [f\"Decision {i+1}: choose GAMBLE or SURE {amt}\" for i, amt in enumerate(sure_amounts)]\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are participating in an economics experiment.\n",
    "\n",
    "About you:\n",
    "- Location: {p.location}\n",
    "- Occupation: {p.occupation}\n",
    "\n",
    "You will make 6 independent decisions.\n",
    "\n",
    "For each decision, choose ONE option:\n",
    "\n",
    "GAMBLE:\n",
    "- 50% chance to receive 300 units\n",
    "- 50% chance to receive 0 units\n",
    "\n",
    "SURE:\n",
    "- Receive the stated amount for sure\n",
    "\n",
    "{lines}\n",
    "\n",
    "Important:\n",
    "- Do NOT explain your choices.\n",
    "- Give your final decisions only.\n",
    "\n",
    "Reply with exactly one line:\n",
    "CHOICES: c1,c2,c3,c4,c5,c6\n",
    "(each ci must be GAMBLE or SURE)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2737a359-3423-40d3-8f13-c3fb6451c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dictator_game(state: DecomposingTrustState):\n",
    "    prompt = dictator_prompt(state)\n",
    "    output = run_llm(prompt)\n",
    "\n",
    "    state.transcript.append({\n",
    "        \"task\": \"dictator_game\",\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "        \"status\": \"completed\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57a6abb1-3390-4c79-bd2f-fb5f7e893553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_triple_dictator_game(state: DecomposingTrustState):\n",
    "    prompt = triple_dictator_prompt(state)\n",
    "    output = run_llm(prompt)\n",
    "\n",
    "    state.transcript.append({\n",
    "        \"task\": \"triple_dictator_game\",\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "        \"status\": \"completed\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b46bce-c354-4ea1-992a-2969176df7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trust_game(state: DecomposingTrustState):\n",
    "    \n",
    "    # TRUSTOR\n",
    "    if state.trust_role == \"trustor\":\n",
    "        prompt = trustor_prompt(state)\n",
    "        output = run_llm(prompt)\n",
    "\n",
    "        sent = tg_extract_scalar_llm(\n",
    "        raw_text=output,\n",
    "        min_value=0,\n",
    "        max_value=100,\n",
    "        target=\"send\",\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "    )\n",
    "    \n",
    "        expected = tg_extract_scalar_llm(\n",
    "            raw_text=output,\n",
    "            min_value=0,\n",
    "            max_value=100 + (3 * sent if sent is not None else 300),\n",
    "            target=\"expect\",\n",
    "            model=model,\n",
    "            tokenizer=tok,\n",
    "        )\n",
    "\n",
    "        state.trust_send_operational = sent\n",
    "        state.trust_expect_operational = expected\n",
    "\n",
    "        state.transcript.append({\n",
    "            \"task\": \"trust_game\",\n",
    "            \"role\": \"trustor\",\n",
    "            \"prompt\": prompt,\n",
    "            \"output\": output,\n",
    "            \"status\": \"completed\",\n",
    "        })\n",
    "\n",
    "    # TRUSTEE (strategy method)\n",
    "    elif state.trust_role == \"trustee\":\n",
    "        prompt = trustee_prompt(state)\n",
    "        output = run_llm(prompt)\n",
    "\n",
    "        schedule = tg_extract_schedule_llm(\n",
    "            raw_text=output,\n",
    "            amounts=[10,20,30,40,50,60,70,80,90,100],\n",
    "            max_receive_multiplier=3,\n",
    "            model=model,\n",
    "            tokenizer=tok,\n",
    "        )\n",
    "\n",
    "        state.trust_return_schedule_operational = schedule\n",
    "\n",
    "        state.transcript.append({\n",
    "            \"task\": \"trust_game\",\n",
    "            \"role\": \"trustee\",\n",
    "            \"prompt\": prompt,\n",
    "            \"output\": output,\n",
    "            \"status\": \"completed\",\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14902f53-2837-4274-9bb9-3f317c1f3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_risk_task(state: DecomposingTrustState):\n",
    "    \n",
    "    prompt = risk_prompt(state)\n",
    "    output = run_llm(prompt)\n",
    "\n",
    "    state.transcript.append({\n",
    "        \"task\": \"risk_task\",\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "        \"status\": \"completed\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "126e0b6b-19e3-40cc-87a2-6152ede4be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_participant(state: DecomposingTrustState):\n",
    "    \n",
    "    run_dictator_game(state)\n",
    "    run_triple_dictator_game(state)\n",
    "    run_trust_game(state)     \n",
    "    run_risk_task(state) \n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a48bafb-8ddc-4f4e-b3bd-b8401a539fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 experiments, dictator, triple dictator, trust game (TG- normal and schedule), and risk task.\n",
    "# out of these, we need to extract ops in real time only for the TG\n",
    "# so we will have 3 LLM extractors. 2 for TG and one common to the other 3\n",
    "\n",
    "def tg_extract_scalar_llm(\n",
    "    raw_text: str,\n",
    "    min_value: int,\n",
    "    max_value: int,\n",
    "    target: Literal[\"send\", \"expect\"],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens: int = 32,\n",
    ") -> int | None:\n",
    "    if target == \"send\":\n",
    "        task = \"the amount the person decided to SEND\"\n",
    "        rule = \"Ignore examples or other numbers.\"\n",
    "    else:\n",
    "        task = \"the amount the person EXPECTS to RECEIVE BACK\"\n",
    "        rule = \"If a range is stated (e.g., 20–30), extract the LOWER bound of the range.\"\n",
    "    \n",
    "    prompt = f\"\"\"From the text below, extract {task}.\n",
    "    \n",
    "    {rule}\n",
    "    Return a single integer.\n",
    "    \n",
    "    Valid range: {min_value}–{max_value}.\n",
    "    If unclear, output NONE.\n",
    "    \n",
    "    Text:\n",
    "    \\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "    \n",
    "    Output exactly:\n",
    "    AMOUNT: <integer>\n",
    "    or\n",
    "    AMOUNT: NONE\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    in_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    gen = out[0][in_len:]\n",
    "    ans = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    m = re.search(r\"AMOUNT:\\s*(NONE|\\d+)\", ans, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    v = m.group(1)\n",
    "    if v.upper() == \"NONE\":\n",
    "        return None\n",
    "\n",
    "    val = int(v)\n",
    "    return val if min_value <= val <= max_value else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36eb1348-2edb-4f20-9632-4d07ee5895d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tg_extract_schedule_llm(\n",
    "    raw_text: str,\n",
    "    amounts: list[int],              # e.g., [10,20,...,100]\n",
    "    max_receive_multiplier: int,     # 3 in this paper\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens: int = 256,\n",
    ") -> dict[int, int] | None:\n",
    "    \n",
    "    # For each x, trustee can return 0..(3*x)\n",
    "    ranges = \", \".join([f\"{x}:0-{max_receive_multiplier*x}\" for x in amounts])\n",
    "\n",
    "    prompt = f\"\"\"From the text below, extract the trustee's return schedule.\n",
    "\n",
    "We need returns for these sent amounts: {amounts}.\n",
    "For each sent amount x, return y must be an integer in [0, {max_receive_multiplier}*x].\n",
    "\n",
    "If the schedule is missing or unclear, output NONE.\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Output exactly one line:\n",
    "SCHEDULE: x=y, x=y, ...\n",
    "or\n",
    "SCHEDULE: NONE\n",
    "\n",
    "Valid per-x ranges: {ranges}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    in_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    gen = out[0][in_len:]\n",
    "    ans = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    m = re.search(r\"SCHEDULE:\\s*(.*)\", ans, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    body = m.group(1).strip()\n",
    "    if body.upper().startswith(\"NONE\"):\n",
    "        return None\n",
    "\n",
    "    # Parse \"10=5, 20=10, ...\"\n",
    "    pairs = re.findall(r\"(\\d+)\\s*=\\s*(\\d+)\", body)\n",
    "    sched = {}\n",
    "    for k, v in pairs:\n",
    "        x = int(k); y = int(v)\n",
    "        sched[x] = y\n",
    "\n",
    "    # Validate completeness + bounds\n",
    "    for x in amounts:\n",
    "        if x not in sched:\n",
    "            return None\n",
    "        y = sched[x]\n",
    "        if not (0 <= y <= max_receive_multiplier * x):\n",
    "            return None\n",
    "\n",
    "    return sched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f601e4-e92d-4c49-a9e4-469e3589438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_extract_amount_llm(\n",
    "    raw_text: str,\n",
    "    min_value: int,\n",
    "    max_value: int,\n",
    "    model,\n",
    "    tokenizer,\n",
    ") -> int | None:\n",
    "    prompt = f\"\"\"Extract the amount the person says they will give from the text.\n",
    "\n",
    "Range: {min_value}-{max_value}.\n",
    "If no clear amount is stated, output NONE.\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Output:\n",
    "AMOUNT: <integer>\n",
    "or\n",
    "AMOUNT: NONE\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    in_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    gen = out[0][in_len:]\n",
    "    ans = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    m = re.search(r\"AMOUNT:\\s*(NONE|\\d+)\", ans, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    v = m.group(1)\n",
    "    if v.upper() == \"NONE\":\n",
    "        return None\n",
    "    val = int(v)\n",
    "    return val if (min_value <= val <= max_value) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd221c5b-6e51-4400-b80c-58ef521a6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_extract_risk_choices_llm(\n",
    "    raw_text: str,\n",
    "    n_choices: int,\n",
    "    model,\n",
    "    tokenizer,\n",
    ") -> list[str] | None:\n",
    "    prompt = f\"\"\"From the text below, extract the participant's choices for {n_choices} decisions.\n",
    "Each choice must be exactly GAMBLE or SURE.\n",
    "If unclear, output NONE.\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Output exactly one line:\n",
    "CHOICES: c1,c2,c3,c4,c5,c6\n",
    "or\n",
    "CHOICES: NONE\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    in_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    gen = out[0][in_len:]\n",
    "    ans = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    m = re.search(r\"CHOICES:\\s*(.*)\", ans, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    body = m.group(1).strip()\n",
    "    if body.upper().startswith(\"NONE\"):\n",
    "        return None\n",
    "\n",
    "    parts = [p.strip().upper() for p in body.split(\",\")]\n",
    "    if len(parts) != n_choices:\n",
    "        return None\n",
    "    if any(p not in (\"GAMBLE\", \"SURE\") for p in parts):\n",
    "        return None\n",
    "    return parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40779cf8-b87e-476b-992b-256c02db31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    offload_folder=\"offload\"\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2f001ec-f8e2-43f1-8459-3fd10f84fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    gen_tokens = output[0][input_len:]\n",
    "    text = tok.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    #print(\"-\" * 50)\n",
    "    #print(\"RAW OUTPUT:\\n\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4505849-6074-421e-80bb-3cb1d5a4815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dictator_game completed\n",
      "   triple_dictator_game completed\n",
      "   trust_game completed\n",
      "   risk_task completed\n"
     ]
    }
   ],
   "source": [
    "smoke_states = []\n",
    "states = init_states(participants)\n",
    "\n",
    "for s in states:\n",
    "    run_participant(s)\n",
    "\n",
    "    #print(s.participant.pid, s.trust_role)\n",
    "    for t in s.transcript:\n",
    "        print(\"  \", t[\"task\"], t[\"status\"])\n",
    "\n",
    "    smoke_states.append(asdict(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbaa8e2-a6f8-4a7e-ae25-20e35c94523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"decomposing_trust.json\", \"w\") as f:\n",
    "    json.dump(smoke_states, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
